Autonomous Vehicles(AVs) are playing an important role in
future transportation. Large companies such as Google,
Uber\cite{1} are racing to develop AVs and some of them have already been deployed on the road. 
AVs rely on different sensors\cite{17} such as cameras, LiDARs, Radars, IMU(Inertial
Measurement Unit) and GPS to know the physical environment and react accordingly. Among them, perception sensors
including cameras and LiDARs provide the obstacle and traffic sign information to AVs to avoid wrong decisions like
collision and violating traffic rules, etc. Therefore, multiple
prior works have been studying the security of these perception sensors.

Prior work has shown that AVs are vulnerable to attacks
towards camera \cite{7, 9, 23} or LiDAR sensors \cite{4, 6, 19, 25}.
Adversaries can change the texture of 2D image \cite{23}(e.g.,
stop sign) or add well-designed adversarial patches\cite{9} to
mislead the cameras. They can also inject laser\cite{6} to spoof
the LiDAR sensors.

To defend the aforementioned attacks, researchers designed
transformation for perceived inputs from camera\cite{13} and LiDAR\cite{22} sensors individually. Multiple Sensor Fusion(MSF)
algorithms are also proposed to integrate inputs from cameras and LiDARs to produce correct output based on the
unattacked sensors.

However, an important and widely accepted assumption in
MSF is there is at least one clean sensor\cite{msf-adv}.

This assumption is approved to fail at specific situations
by Cao et al. \cite{msf-adv}. They observed that different shapes of a 3D
object can spoof both LiDARs and cameras with the change
of point position and pixel valuesc\cite{msf-adv}. Also, the generated 3D
objects are stealthier and more robust than previous work. As
no safe sensor exists under this attack, the MSF algorithm doesn’t
have reliable sensors to trust. Most recovering-based defenses only work in one kind of sensor attack and thus fail to
have good performance under this scenario. Therefore, 
in the experiments, the car didn’t detect the adversarial obstacles and crashed into it.

We observe that defense against attacks to both cameras
and LiDARs is urgently needed to secure AVs. Even though
some prior works\cite{if-defense, 22, 24} have studied 3D objects adversarial defenses. They are targeting pure neural networks
instead of a real AV system. The object detection in a real AV
system consisting of input pre-processing(e.g., format transformation, feature generation, etc.), neural network model,
post-processing(e.g., clustering, multiple sensor fusion, etc.).
Also, static and general 3D objects are considered in prior
works rather than moving and traffic-related objects. Thus,
their methods can’t be directly applied to the attack we’re
targeting.

Our motivation comes from the previous defenses that remove perturbations from the corrupted inputs\cite{8, 14},
 or take the majority vote
from randomly transformed image\cite{12} against2D images and 3D objects adversarial attacks. 
The main idea behind these defenses is to do transforms on
the perception input and remove the malicious characteristics
generated by the attacker. 

Therefore, in this work, we aim to smooth the noisy surface of obstacles with this latest 3D point cloud reconstruction network - IF-Defense\cite{if-defense}.
IF-Defense\cite{if-defense} aims to recover the surface of 3D objects with the awareness of geometry property and uniform distribution of the points.
However, it's only trained and tested in the dataset containing single 3D objects.
Its performance in working in real AV perception is unknown.
Moreover, directly applying IF-Defense\cite{if-defense} may result in unnecessary computational overhead as 
not all the 3D perception needs recovery. 
For example, in a wide open area, the majority of the perception is road and only a small part is the obstacles.

Based on this, we further propose a lightweight segmentation algorithm, aiming to provide a rough location for the areas to be recovered.
The intuition for this algorithm is, we observe that due to laser imaging in the LiDAR system, a blank shadow is formed after the obstacle.
Usually, the number of points in the obstacle area is much larger than that in road areas.
And the number of points in shadow areas is much smaller.
We then calculate the Manhattan distance between obstacle areas and shadow areas to relate the obstacle with the corresponding shadow.
A new set of point clouds containing the obstacle and its shadow is treated as the object to be recovered and sent to IF-Defense\cite{if-defense}.
Through this, we aim to recover the noisy surface of 3D objects in the point cloud form.

% In the context of attacks towards cameras and LiDARs in
% real AVs, similar malicious features also exist. These features
% can be obtained in a white-box setting, namely, the system has
% some knowledge about the attackers’ strategy. For example,
% the optimization methods used by the attacker may lead to
% unsmoothiness and discontinuity in the color or shape of the
% objects. Applying filters can make the inputs more smooth
% and decrease the noise introduced by the attacker.

One advantage that our work has is, we are not aimed
to recover the direct output for both LiDAR and camera object detection models.
We only need to provide at least one or a few correct outputs and let the
MSF fuse the two results from LiDAR and camera model to correct detection results. 
Even though we can’t guarantee to recover the outputs, at least some benign inputs
are sent to MSF and there are fewer possibilities in the wrong detection.

In this work, we aim to answer the following research
questions:

1. Is it possible to apply transforms on noised 3D objects
to recover the detection output of the AV system?

2. Will the transforms degrade the accuracy when it’s applied to clean inputs?

3. Can the adversaries alter their attack accordingly to avoid
this model-specific defense?

\subsection{Threat Model}
The attacker is assumed to know the details of the MSF algorithm
in the victim system. Most adversarial attacks\cite{4, 7, 9, 19, 23}
on camera or LiDAR sensors in the AVs are white-box attacks
and this assumption holds for many prior works. The adversaries are also able to profile the road environment they’re
targeting, and generate obstacles using 3D printing.

\subsection{Challenges}
\textbf{C1: How to find useful transforms for both camera and
LiDAR perception?}
Prior works have studied the transformations in 3D objects
for camera or LiDAR perception [20, 22, 24]. However, due
to the different perception theories in camera and LiDAR, it
might be hard to find a common transform for both of them.
And this is needed in the attacks towards both sensors.

\textbf{C2: How to avoid the deterioration of performance on clean objects?}
Applying random transform on all inputs will decrease the
accuracy rate on clean inputs [12], as it may remove some
important properties in real-life objects. Therefore, the transformation that we’re designing should try to recover the corrupted
inputs while remain the important properties in clean inputs.

\textbf{C3: How to avoid the attacker from altering the attacks
with the knowledge of the transform?}
Transformation is usually effective to a certain series of
characteristics in the inputs. If a white-box attacker knows the
transforms applied in the system, he may alter the optimization objectives to prevent the transform from removing the
adversarial parts. Is there a way to decrease the success rate
that the attacker can design a similar attack?


